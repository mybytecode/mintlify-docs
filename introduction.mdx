---
title: Introduction
description: "Plug, Play, and Power Your Workflows with AI."
---

Welcome to the documentation for our AI platform — your all-in-one hub for deploying, testing, and managing Model Context Protocol (MCP) servers. This platform is built for developers, teams, and enterprises looking to streamline how they define, interact with, and govern AI-powered tools and workflows.


## What is MCP?

The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you're building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.

##  Platform Highlights
### Deploy MCP Servers Easily
Get started in minutes using one of three flexible deployment paths:

- **Curated Directory**
Browse a collection of verified MCP servers shared by trusted developers from our community.

- **Bring Your Own Server**
Already have a GitHub-hosted MCP server? Just link your repo and deploy directly.

- **Build with OpenAPI**
Define your own MCP server using OpenAPI specifications — no need to write additional backend logic.

### Playground for Multi-LLM Testing
Test your MCP servers with different large language models (LLMs) in our interactive playground. Compare outputs, tweak prompts, and evaluate behavior across model providers in a single place.

### Logging and Analytics (Sentry for MCPs)
Monitor and understand your MCPs using our open-source observability library. All logs and usage data are centralized on the platform, offering real-time insight and historical analysis — built like Sentry, but purpose-built for MCPs.

### Tool-Level Authorization
Maintain strict governance over who can access what:

- Admins can define access policies at the team or individual level.
- Control visibility and usage of MCP servers and tools across departments or clients.

### Who Should Use This?
This platform is perfect for:

- Developers building tools that integrate with LLMs
- Teams collaborating on AI workflows across contexts
- Enterprises needing modularity, traceability, and access control in their AI systems